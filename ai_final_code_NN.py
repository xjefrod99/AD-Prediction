# -*- coding: utf-8 -*-
"""AI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jDxBWUFsXnsiZUzjPvwVD-S0cHNLk-kW
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow import keras

from sklearn.preprocessing import StandardScaler

from sklearn.model_selection import train_test_split
import random
import tensorflow as tf
from tensorflow import keras

from keras.models import Sequential
from keras.layers import Flatten, Dense
from keras import Input
from keras import Model
#from keras.estimator import DNNClassifier
import seaborn as sns
from sklearn.decomposition import PCA

#now doing the same thing only on 3k features
df0 = pd.read_csv('samples_org_dataset_3kfeatures.csv', header=0, index_col = 0)
df = df0.copy()
del df['donor_name']
del df['specimen_name']
del df['hemisphere']
del df['nincds_arda_diagnosis']

del df['specimen_id']
del df['structure_id']
del df['donor_id']

del df['polygon_id']
del df['rna_well_id']

del df['apo_e4_allele']

del df['ever_tbi_w_loc']

del df['structure_name']
del df['structure_acronym']

del df['structure_color']




random.seed(4)
train, test = train_test_split(df, 
                               test_size=0.30, 
                               random_state=42, 
                               shuffle=True)


y1 = train['Dementia']
y2 = train['Alzheimer']
del train['Dementia']
del train['Alzheimer']
scaler = StandardScaler()
train.iloc[:,0:-1] = scaler.fit_transform(train.iloc[:,0:-1].to_numpy())


yy1 = test['Dementia']
yy2 = test['Alzheimer']
del test['Dementia']
del test['Alzheimer']
test.iloc[:,0:-1] = scaler.fit_transform(test.iloc[:,0:-1].to_numpy())

model = tf.keras.Sequential([ keras.layers.Dense(units=1, input_shape=[3392])])
#model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation=tf.nn.relu), tf.keras.layers.Dense(10, activation=tf.nn.softmax)])
#model = tf.keras.models.Sequential([ tf.keras.layers.Dense(400, activation=tf.nn.relu), tf.keras.layers.Dense(units=1, input_shape=[3392])])
#model.add(Dense(1, activation='sigmoid')) 

loss_function_used = 'binary_crossentropy' # or use categorical_crossentropy, sparse_categorical_crossentropy,binary_crossentropy

#model.compile(optimizer='sgd', loss= loss_function_used, metrics=['accuracy'])
model.compile(loss=loss_function_used, optimizer=tf.keras.optimizers.Adam(learning_rate=0.00025), metrics=['accuracy'])

#train a model on y1 = dementia or y2 = Alzheimer
model.fit(train, y2, epochs=200)
model.evaluate(test, yy2)

model.evaluate(test, yy2)

learning_rate = [0.00025,0.0003, 0.0004, 0.00045,  0.001] #[0.01, 0.001, 0.005, 0.0075 , 0.0001, 0.00025, 0.0005 ]
best_max = 0.6
epoch_nums = [150, 200, 250, 300, 400]
loss_function_used = ['categorical_crossentropy','binary_crossentropy' ]
opts = [0, 1]
for opt in opts:
  for lr in learning_rate:
    for loss_f in loss_function_used:
      if opt == 0:
        op = tf.keras.optimizers.Adam(learning_rate=lr)
      else:
        op = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.0, nesterov=False, name="SGD")
    
      model = tf.keras.models.Sequential([ tf.keras.layers.Dense(400, activation=tf.nn.relu), tf.keras.layers.Dense(units=1, input_shape=[3392])])
      model.add(Dense(1, activation='sigmoid')) 

      model.compile(loss='binary_crossentropy', optimizer=op, metrics=['accuracy'])

      #train a model on y1 = dementia or y2 = Alzheimer
      for e in epoch_nums:
        model.fit(train, y2, epochs=e)
        l0, tmax = model.evaluate(train, y2)
        l, max_ = model.evaluate(test, yy2)
        if max_ > best_max :
          #only update the max if its greater than the one we saw, also save the model and best parameters
          best_max = max_
          best_model = "Adam" if opt == 0 else "SGD" 
          best_l =lr
          Best_e = e
          best_loss = loss_f

print("best model seen so far is",  best_model, " with loss(f) = ", best_loss, "epochs : ", Best_e, "@ a learning rate of lr", best_l, 'accuracy of ', best_max)

op = tf.keras.optimizers.Adam(learning_rate=0.0004)
#model = tf.keras.Sequential([ keras.layers.Dense(units=1, input_shape=[3392])])
model = tf.keras.models.Sequential([ tf.keras.layers.Dense(350, activation=tf.nn.relu), tf.keras.layers.Dense(units=1, input_shape=[3392])])
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer=op, metrics=['accuracy'])

model.fit(train, y2, epochs=150)
model.evaluate(test, yy2)

# Commented out IPython magic to ensure Python compatibility.
from re import template
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
sns.set()
best_max = 0.7
scores = []
learning_rate = [0.0003, 0.00038, 0.0004,0.001]
for lr in learning_rate:
  op = tf.keras.optimizers.Adam(learning_rate=lr)
  #model = tf.keras.Sequential([ keras.layers.Dense(units=1, input_shape=[3392])])
  model = tf.keras.models.Sequential([ tf.keras.layers.Dense(400, activation=tf.nn.relu), tf.keras.layers.Dense(units=1, input_shape=[3392])])
  model.add(Dense(1, activation='sigmoid')) 

  model.compile(loss='binary_crossentropy', optimizer=op, metrics=['accuracy'])

  temp = model.fit(train, y2, epochs=200, validation_data=(test, yy2), batch_size=100)
  l, max_ = model.evaluate(test, yy2)
  if max_ > best_max:
    best_max = max_
    hist = temp
  scores.append( (lr, max_))
  

print(scores)
acc = hist.history['accuracy']
val = hist.history['val_accuracy']
epochs = range(1, len(acc) + 1)
 
plt.plot(epochs, acc, '-', label='Training accuracy')
plt.plot(epochs, val, ':', label='Validation accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.plot()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import seaborn as sns
sns.set()
best_max = 0.6
scores = []

# Select the ones you want
train_feature_important= train[feature_important ]


test_feature_important= test[feature_important ]

scores = []
learning_rate = [0.0001, 0.00035, 0.0004,0.001]

for lr in learning_rate:
  op = tf.keras.optimizers.Adam(learning_rate=lr)
  #model = tf.keras.Sequential([ keras.layers.Dense(units=1, input_shape=[3392])])
  model_feature_important = tf.keras.models.Sequential([ tf.keras.layers.Dense(300, activation=tf.nn.relu), tf.keras.layers.Dense(units=1, input_shape=[1500])])
  model_feature_important.add(Dense(1, activation='sigmoid')) 

  model_feature_important.compile(loss='binary_crossentropy', optimizer=op, metrics=['accuracy'])

  temp = model_feature_important.fit(train_feature_important, y2, epochs=200, validation_data=(test_feature_important, yy2), batch_size=100)
  l, max_ = model_feature_important.evaluate(test_feature_important, yy2)
  if max_ > best_max:
    best_max = max_
    hist = temp
  scores.append( (lr, max_))

print(scores)
acc = hist.history['accuracy']
val = hist.history['val_accuracy']
epochs = range(1, len(acc) + 1)
 
plt.plot(epochs, acc, '-', label='Training accuracy')
plt.plot(epochs, val, ':', label='Validation accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.plot()

from sklearn.ensemble import ExtraTreesClassifier
import matplotlib.pyplot as plt

modelf2 = ExtraTreesClassifier()
modelf2.fit(train,y2)
print(modelf2.feature_importances_) #use inbuilt class feature_importances of tree based classifiers
#plot graph of feature importances for better visualization
feat_importances = pd.Series(modelf2.feature_importances_, index=train.columns)
feat_importances.nlargest(20).plot(kind='barh')
modelf2.score(test, yy2)
plt.title('Feature Importance')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.plot()

print(sum(y2)/len(y2))
print(sum(yy2)/len(yy2))

pca = PCA(.95) #choosing the features that contribute to 95% of var
print(np.shape(train))
train_new = scaler.fit_transform(train)

train_new = pca.fit_transform(train_new) #using principal component analysis

#finding out how much of the variance each pca explains
var = pca.explained_variance_ratio_

print(np.shape(train_new))


# yy1 = test['Dementia']
# yy2 = test['Alzheimer']
# del test['Dementia']
# del test['Alzheimer']
test_new= scaler.fit_transform(test)

test_new = pca.transform(test_new) #using principal component analysis

print(abs( pca.components_ ))
#we can know how much each feature contributes to each pca
pca_contribs = abs( pca.components_ )

fimp = feat_importances.nlargest(2000)
feature_important = list(fimp.index.values) #1000 most important features
print(len(feature_important))

"""best model seen so far is Adam with loss(f) =  sparse_categorical_crossentropy epochs :  150 @ a learning rate of lr 0.001 accuracy of  0.7368420958518982"""

def myplot(score,coeff,y, labels=None):
    xs = score[:,0]
    ys = score[:,1]
    n = coeff.shape[0]
    scalex = 1.0/(xs.max() - xs.min())
    scaley = 1.0/(ys.max() - ys.min())
    plt.scatter(xs * scalex,ys * scaley, c = y)
    for i in range(n):
        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 1.0)
        if labels is None:
            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, "Var"+str(i+1), color = 'g', ha = 'center', va = 'center')
        else:
            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')

plt.xlim(-1,1)
plt.ylim(-1,1)
plt.xlabel("Principal Component{}".format(1))
plt.ylabel("Principal Component{}".format(2))

plt.set_title('PCA of 3', fontsize = 20)
plt.grid()

#Call the function. Use only the 2 PCs.
myplot(train_new[:,0:2], np.transpose(pca.components_[0:2, :]),y2 )
plt.show()

pca = PCA(0.95) #choosing the features that contribute to 95% of var
print(np.shape(train))
train_new = scaler.fit_transform(train)

train_new = pca.fit_transform(train_new) #using principal component analysis

#finding out how much of the variance each pca explains
var = pca.explained_variance_ratio_

print(np.shape(train_new))

test_new= scaler.fit_transform(test)

test_new = pca.transform(test_new) #using principal component analysis

print(abs( pca.components_ ))
#we can know how much each feature contributes to each pca
pca_contribs = abs( pca.components_ )

learning_rate = [ 0.00025,0.0003, 0.00035, 0.0004, 0.00045, 0.0005, 0.001] 
best_max = 0.0
scores = [] 
for lr in learning_rate:
  op = tf.keras.optimizers.Adam(learning_rate=lr)
  #model = tf.keras.Sequential([ keras.layers.Dense(units=1, input_shape=[3392])])
  model_feature_important = tf.keras.models.Sequential([ tf.keras.layers.Dense(300, activation=tf.nn.relu), tf.keras.layers.Dense(units=1, input_shape=[215])])
  model_feature_important.add(Dense(1, activation='sigmoid')) 

  model_feature_important.compile(loss='binary_crossentropy', optimizer=op, metrics=['accuracy'])

  temp = model_feature_important.fit(train_new, y2, epochs=200, validation_data=(test_new, yy2), batch_size=100)
  l, max_ = model_feature_important.evaluate(test_new, yy2)
  if max_ > best_max:
    best_max = max_
    hist = temp
  scores.append( (lr, max_))

print(scores)
acc = hist.history['accuracy']
val = hist.history['val_accuracy']
epochs = range(1, len(acc) + 1)
 
plt.plot(epochs, acc, '-', label='Training accuracy')
plt.plot(epochs, val, ':', label='Validation accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.plot()

"""best model seen using PCA, so far is Adam with loss(f) =  sparse_categorical_crossentropy @ a learning rate of lr 0.00025 accuracy of  0.640350878238678"""

variance = pca.explained_variance_ratio_
index = [i for i in range(263)]
print((variance))

sns.scatterplot(x=index,y = variance, sizes=(20, 200))

plt.title('Variance of 263 Principal Components')
plt.xlabel('PCA #')
plt.ylabel('Variance')
plt.legend(loc='lower right')
plt.grid()
plt.plot()

#print(abs( pca.components_ ))



ans = []
for i in range(10): #looking in the first 10 principal components
  pca_important = pd.Series( abs( pca.components_[i,:] ))
  huh = pca_important.nlargest(10) #finding 10 largest values
  which_features = list(huh.index.values) #naming
  colname = list(train.columns[which_features])
  
  ans += (colname)













print(ans)

f = open("PCA_important.txt", "w")

for a in ans:
  f.write(a + " \n")

f.close()



#get correlations of each features in dataset

train_feature_important= train[feature_important ]

dataset_small = train_feature_important.merge(y2.to_frame(), left_index=True, right_index=True)
corrmat = dataset_small.corr()
top_corr_features = corrmat.index
plt.figure(figsize=(20,20))
#plot heat map
g=sns.heatmap(dataset_small[top_corr_features].corr(),annot=True,cmap="RdYlGn")